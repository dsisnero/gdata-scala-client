#summary A guide for people who want to know more about the implementation.
#labels Phase-Implementation

= Introduction =

This document is intended to give a thorough view of the design and implementation of the Google Data Scala client library. If you want to know more about how this library works, or contribute to the project, this is the starting point. It assumes the reader is familiar with [http://scala-lang.org Scala], the [http://code.google.com/apis/gdata/index.html Google Data API protocol], [http://www.atomenabled.org/developers/syndication/atom-format-spec.php Atom] and the [http://ietfreport.isoc.org/idref/draft-ietf-atompub-protocol/ Atom Publishing Protocol].

_Note: The Scala client library needs a working [http://scala-lang.org Scala] installation newer than 2.7.0-final. That means a [http://www.scala-lang.org/downloads/distrib/files/nightly/ nightly build], until a bug fix release is issued to deal with [https://lampsvn.epfl.ch/trac/scala/ticket/632 this bug]._

= Setup =

This section describes the steps to take if you want to start developing the library. It describes how to checkout the sources, build and run the tests. Everything is straight forward, so most readers used to [http://scala-lang.org Scala] development can skip this section and come back if needed.

Before you start, make sure you have installed the following software:

 * An [http://subversion.tigris.org/ svn] client.
 * [http://ant.apache.org ant]
 * A [http://scala-lang.org Scala] distribution greater than 2.7.0.
 * [http://emma.sourceforge.net/ emma] for code coverage (optional).

== Setting up your working copy ==

Follow the instructions on the [http://code.google.com/p/gdata-scala-client/source/checkout project website] to checkout a working copy. Then `cd` into the directory and type
{{{
$ ant build
}}}
Make sure the environment variable `SCALA_HOME` points to your Scala installation.

To build and run the tests, type
{{{
$ ant test
}}}

The build process can be configured by setting the appropriate values in `build.properties`. If you have installed [http://emma.sourceforge.net/ emma], make sure you update the properties file to point to its installation directory:
{{{
emma.dir=/usr/local/soft/emma-2.0.5312/lib
}}}

Now you can get an html report of test coverage:
{{{
$ ant coverage
}}}

= Library Overview =

The Scala client library handles XML serialization, HTTP connection, authentication and query building for Google Data. The library is centered around serializing and deserializing XML, and provides a core set of classes called _Picklers_, on which all the serialization code is based. The object model is decoupled from serialization code, and the library provides classes for Atom and Google Data common elements. Supported services, like !YouTube or Calendar, define new model classes and picklers, using exactly the same approach as the core library. Users that need to extend the library (for a new service, or to accommodate an extension to an existing service) can use the same approach.

The library provides support for making queries and updating data through HTTP. The approach is straight forward, and similar to the road taken by the [http://code.google.com/p/gdata-java-client/ Java client library]. Users connect to Google services through `Service` objects, that provide methods for making queries and updating data on the server. Queries are encoded as specific URLs, and the Query builder provides a natural syntax for expressing complex queries. These are the objects that are most visible to the users of this library.

== Conventions ==

The library goes to great lengths to provide a type-safe layer on top of the underlying XML protocol. Whenever possible, Scala data types are used to represent the underlying data. Here is a high level view of the conventions followed throughout the library:
 * whenever a piece of data is optional, it is represented as an `Option[T]`. *null* values should never be valid values.
 * whenever a default value is specified for an element or attribute, such data is represented as a value of the right type, and the default value is filled in when parsing.
 * all data is mutable. This is in order to support update (although some services are read-only).
 * serialization code is separated from data representation. Picklers (the name for serialization code) are usually found as values in the companion object of data classes.
 * errors in the HTTP transport layer are signaled as exceptions.
 * convenience methods and constructors are provided for the most common cases. For instance, an `EventEntry` has a constructor taking a title and a description as plain strings (although they could be `html` or `xhtml`) and two `DateTime` objects (although an event might have only one or none).
 * names in data classes closely follow the XML data they model. If an [http://code.google.com/apis/gdata/elements.html#gdEventKind event entry] has an element called `eventStatus`, the `EventEntry` class will have a field named the same. This makes it easy to use existing Google documentation.

== Organization ==

Along with the core classes for XML serialization (_picklers_), there are classes that model the various data used by the Google Data protocol (like feeds, contacts, calendar entries, etc) and HTTP connection classes. The library is split in packages along the following lines:
  * `com.google.xml.combinators` The XML serialization core
  * `com.google.gdata.data` Google Data common classes, including the [http://atomenabled.org/developers/syndication/atom-format-spec.php Atom protocol] classes.
    * `kinds` Google [http://code.google.com/apis/gdata/elements.html kinds] classes (common data classes used by several Google services).
    * `media` [http://search.yahoo.com/mrss Media Rss] classes (used by the !YouTube service).
    * `util` various common classes like `DateTime` or `NormalPlayTime`
  * `com.google.gdata.client` Google Data HTTP connection and authentication handling.
  * `com.google.gdata.<servicename>` specific data classes and services for each implemented service, like Calendar or !YouTube.

= XML Pickler Combinators =

The serialization code is built around [http://research.microsoft.com/~akenn/fun/picklercombinators.pdf pickler combinators]. A _pickler_ is an object that can serialize and deserialize some type. The library provides implementations for base cases, and _combinators_ for assembling more sophisticated picklers out of simple ones. This should strike a familiar note if you've ever used the [http://www.scala-lang.org/docu/files/api/index.html combinator parsing] library in the Scala distribution.

Pickler combinators are implemented in `com.google.xml.combinators`. The `Pickler` interface needs just two methods:
{{{
  abstract class Pickler[A] {
    def pickle(v: A, in: XmlOutputStore): XmlOutputStore
    def unpickle(in: XmlInputStore): PicklerResult[A]
  }
}}}
The input and output types are abstractions over the XML representation. The result type of the unpickle method is a `PicklerResult`, which can be either `Success` or `Error`. All errors are issued through `PicklerResult` values, as this allows combinators to decide later if an error should fail the whole pickler, or try another alternative.

== Primitive picklers ==

The library provides picklers for primitive types. It is interesting to note that they do not commit to an attribute or an element content. Combinators will decide that when they wrap one of the basic picklers.
 * `text` This is a pickler for Strings. 
 * `boolVal` This is a pickler for Booleans.
 * `intVal` This is a pickler for Integers.
 * `doubleVal` This is a pickler for Double values.
 * `dateTime` This is a pickler for date/time in RFC 3339 format.

== Combinators ==

Combinators are functions that take as arguments one or more picklers, and return a pickler for a more complex data type. Sometimes they return a pickler for the same data type as their argument, but perform some processing on input, like `attr` and `elem`. To keep the exposition clear, descriptions of picklers will talk from the point of view of _unpickling_ (parsing the input), but keep in mind that the pickling part is always implied:

 * `elem(label, p)` Wraps the given pickler in an element with the given `label`. For instance, `elem('id', text)` creates a pickler that accepts strings inside an element called 'id': `<id>Test</id>`. Namespaces can be specified as well (see the API documentation).
 * `attr(label, p)` Wraps the given pickler in an attribute with the given `label`.
 * `seq(pa, pb)` The two picklers are applied in sequence. This combinator is usually written as `pa ~ pb`. The `~` operator is a convenience method defined in the `Pickler` interface, so that sequences can be written using an infix operator.
 * `interleaved(p)`. Makes the given combinator accept input sequences in any order. Unknown elements are ignored. This is used heavily throughout the library.
 * `opt(p)` Turns the given pickler into an optional pickler. If `p` fails, `opt(p)` succeeds with a value of `None`.
 * `rep(p)` Repeatedly apply `p`, until it fails. It results into a list of values unpickled successfully by `p`.
 * `wrap` A combinator used to adapt one type to another. It has a dedicated section below.
 * other, less common combinators. See the scaladoc.

== Example ==

All this has been pretty abstract, so now we turn to a simple example. Let's say we need to write a pickler for Google [http://code.google.com/apis/gdata/elements.html#gdRating rating] elements, with the following schema:
{{{
rating =
   element gd:rating {
      gdCommonProperties,
      attribute rel { xs:string }?,
      attribute value { xs:int }?,
      attribute average { xs:float }?,
      attribute min { xs:int },
      attribute max { xs:int },
      attribute numRaters { xs:int }?
   }
}}}

We start by defining a class for ratings:
{{{
case class Rating(average: Option[Double], 
    min: Int, 
    max: Int, 
    numRaters: Option[Int],
    rel: String,
    value: Option[Int]) {
}
}}}

We follow the conventions and turn all elements and attributes into fields with the same name. Optional elements get an `Option` type. Next we need to define a pickler for ratings in the companion object:
{{{
object Rating {
  import Uris.gdNs
  import Picklers._

  def pickler: Pickler[Rating] = 
    elem("rating", 
          opt(attr("average", doubleVal))
        ~ attr("min", intVal)
        ~ attr("max", intVal)
        ~ opt(attr("numRaters", intVal))
        ~ default(attr("rel", text), "overall")
        ~ opt(attr("value", intVal)))(gdNs)
}
}}}
The pickler definition reads almost like an RNG schema for the rating element. The `default` combinator provides a value in case the attribute _rel_ is not present. The last argument to `elem` is the element namespace (in this case, the Google data namespace).

If you try to compile this code, you'll notice there is a type error: The return type of `pickler` is not `Pickler[Rating]`, but some complex involving `~`. The reason has to do with the sequence combinator, which returns picklers for a pair-like type formed by the two picklers:
{{{
def seq[A, B](pa: => Pickler[A], pb: => Pickler[B]): Pickler[~[A, B]]
}}}
As in the combinator parsing library, `~` is both a convenience method in trait `Pickler`, and a holder class. To fix this error, we need the `wrap` combinator

== Wrap ==

We can fix our code by giving the library a way to transform between our type to the type it understands. The `wrap` combinator does just that: Given a pickler for some type `A`, and two functions `f: A => B` and `g: B => A`, it gives back a pickler for type `B`.
{{{
  def wrap[A, B](pb: => Pickler[A])(g: A => B)(f: B => A): Pickler[B]
}}}

{{{
  def rawPickler = // the previous definition

  def pickler: Pickler[Rating] =
    wrap (rawPickler) {
      case avg ~ min ~ max ~ numRaters ~ rel ~ value => Rating(avg, min, max, numRaters, rel, value)
    } (fromRating)

  private def fromRating(r: Rating) = 
    (new ~(r.average, r.min) ~ r.max ~ r.numRaters ~ r.rel ~ r.value)
}}}

Thanks to type inference, we got away easy: all types are filled in by the compiler. The first argument to `wrap` is the raw pickler, the second one is a function (using Scala's support for patterns as partial functions) that constructs `Rating` objects out of the pair-like structure. The last argument breaks a `Rating` object into pairs.

Oh, and one more thing: because `Rating` is a case class, we can use the constructor functions generated automatically to get a much cleaner definition:
{{{
  def rawPickler = // as before

  def pickler: Pickler[Rating] =
    wrap (rawPickler) (Rating.apply) (Rating.unapply)
}}}

This code uses implicit conversions behind the scenes to adapt the given functions to the expected types. There is one sad thing, though: [http://lampsvn.epfl.ch/trac/scala/ticket/508 ticket #508]. For the moment, the `unapply` method cannot be used.

== Extensions ==

The Google Data protocol is highly extensible. Most elements can be extended with new attributes or elements, and the library provides a solution based on picklers. The basic idea is to collect any unparsed content of an element and store it as XML. An extension is then just another pickler, which combined with an extensible element operates on the collected data. Here's an example involving the Atom `link` element.
{{{
case class Link(href: String, 
    rel: Option[String],
    tpe: Option[String],
    hrefLang: Option[String],
    title: Option[String],
    length: Option[String]) extends HasStore
    
    
object Link {
  implicit val nsAtom = Uris.atomNs
  
  val contentsPickler: Pickler[Link] = wrap(attr("href", text) 
        ~ opt(attr("rel", text))
        ~ opt(attr("type", text))
        ~ opt(attr("hrefLang", text))
        ~ opt(attr("title", text))
        ~ opt(attr("length", text))) (Link.apply) (toPair)
  
  lazy val pickler: Pickler[Link] = elem("link", makeExtensible(contentsPickler))
}}}

The interesting thing to note is the call to `makeExtensible` on the `contentsPickler`. This combinator simply stores whatever was not parsed by the given pickler into a field of the `Link` class. Note that `Link` extends `HasStore`, a trait that declares a `store` field for that purpose. Remember that, even though we talk only about parsing, the store goes both ways: when pickling a `Link`, all unknown elements are pickled too.

Suppose link elements are extended with a child element called `webContent`:
{{{
extend(pickler, elem("webContent", text)(Uris.gCalNs))
}}}
This returns a pickler that handles the additional element.

This scheme works fine for simple cases, but when a class contains objects of a class that was extended, this scheme will lose type information: the container will refer to its members by a super type. This is the case with feeds and entries, and the solution is described in a dedicated section.

= Common Data Classes =

The [http://www.atomenabled.org/developers/syndication/atom-format-spec.php Atom syndication protocol] defines common constructs and the basic structure of feeds and entries. Their model classes and picklers are found under `com.google.gdata.data`, and follow the pattern described above with one exception: feeds and entries, which are described in the next section. The library uses a custom `DateTime` class, whose definition and picklers are found in `com.google.gdata.data.util`. This class handles time zones and parses dates in the RFC 3339 format.

Google defines a number of common classes, and their implementation is found in package `com.google.gdata.data.kinds`. Implementation is straight forward. The interesting cases are `FeedLink` and `EntryLink`, which are special because they might enclose a feed or entry element, and are described below.

= Feeds and Entries =

Feeds are at the center of the Google Data API. Each kind of data that is published by a service is represented as a feed. A feed contains _entries_ along with metadata (such as author, id, or publish date). In turn, each entry represents a specific kind of data, like videos, events, messages. As such, entries are by far the most extended structure in Google Data.

Going back to the extensibility issue, when modeling Atom feeds, we are faced with the choice of a type for entries. Our first attempt might look like this:
{{{
  class AtomFeed {
    var author: String
    ...
    var entries: List[AtomEntry]
  }
}}}

But what happens when we implement !YouTube video feeds? We will extend `AtomEntry` to define `VideoEntry`, but the feed will still 'know' only about `AtomEntry`, and user code would need to down cast. Worse, feeds are often interconnected: a video entry has a comments feed, a related video feed and a user profile feed. All these have different type of entries, who can in turn be extended later. Redefining each feed for each extension (and the transitive closure of its uses) is clearly not a scalable solution. 

In turn, we abstract over the type of entries, and let feeds and entries evolve independently, combining them using mixin composition.

== The Cake Pattern ==

The [http://scala.sygneca.com/patterns/component-mixins cake pattern] is used when different components need to abstract over their dependencies, evolve independently and do so in a type safe way. In our case, feeds and entries are the components that need to inter-operate, but the dependencies should be hard coded in either of them. We start by defining a trait for Entries:
{{{
trait Entries {
  type Entry <: HasStore
  
  def entryPickler: Pickler[Entry] = elem("entry", makeExtensible(entryContentsPickler))(Uris.atomNs)
  def entryContentsPickler: Pickler[Entry]
}
}}}

This component provides an abstract type `Entry`: all components using entries use this abstract type for referring to an entry. This allows them to work with different implementations of `Entries`. It also provides a pickler for this abstract type, expressed in terms of a pickler for entry contents. The `entryContentsPickler` is a method that needs to be defined by concrete implementations, and which should remain abstract until the type is fixed to a concrete type.

Next we look at a component for feeds. Since feeds depend on entries, we'll use a self type annotation to express this requirement:
{{{
trait Feeds { this: Feeds with Entries =>
  type Feed <: Seq[Entry] with HasStore
  
  def feedPickler: Pickler[Feed] = elem("feed", makeExtensible(feedContentsPickler))(Uris.atomNs)
  
  def feedContentsPickler: Pickler[Feed]
}}}

This component is very similar to the previous one, except for the self type annotation, that reads like 'all instances that mix in `Feeds` should also mix in `Entries`'. This allows `Feeds` to be defined in terms of the abstract type `Entry`, for instance by making them implement `Seq[Entry]`.

== Atom Feeds ==

Once we have defined feed and entry components, we can refine them to model the most basic feeds: atom feeds. At each step, we need to refine the abstract type and define a contents pickler for the new bound:
{{{
trait AtomEntries extends Entries {
  type Entry <: AtomEntry
  
  class AtomEntry extends AnyRef with LinkNavigation with HasStore {
    var authors: List[Person] = Nil
    var categories: List[Category] = Nil
    // ...
  }

  lazy val atomEntryContentsPickler: Pickler[AtomEntry] = wrap (...) ({
    case authors ~ cats ~ .. =>
      (new AtomEntry).fillOwnFields(authors, cats, content, contribs, id, links, published, 
          rights, src, summary, title, updated)
  }) (fromEntry)
}
}}}

This new component provides a more specific type of entries, and defines the `AtomEntry` class along with its pickler. Note that the contents pickler method is *not* implemented yet: instead, a `atomEntryContentsPickler` is provided. This allows future extensions, that can reuse the atom entry pickler. Similarly, atom feeds follow:
{{{
trait AtomFeeds extends Feeds { this: AtomFeeds with Entries =>
  type Feed <: AtomFeed with HasStore

  class AtomFeed extends AnyRef with Seq[Entry] with HasStore {
    var authors: List[Person] = Nil
    // ..

    var entries: List[Entry] = Nil
  }

  lazy val atomFeedContentsPickler: Pickler[AtomFeed] = 
    wrap (interleaved(rep(atomPerson("author")) ~ rep(entryPickler))) ({
      case authors ~ entries => new AtomFeed(...)
  }) (fromAtomFeed)
}
}}}
Notice how the pickler is using the abstract method `entryPickler` to handle the abstract `Entry` type.

== Tying the knot ==

To bring everything together, we need to come up with a concrete class that can be instantiated. This means fixing the abstract types, and their picklers. 

== References to other feeds ==

== Conventions ==

== FeedLink and EntryLink ==

= Implementing a new service =

== Implementing new entries ==

= Http and Google Data requests =

